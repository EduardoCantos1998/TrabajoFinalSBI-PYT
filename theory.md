# Theory
Protein–protein interactions (PPIs) are central to biological systems; and predicting the interacting residues is useful for constructing PPI networks, analysing mutations, drug design, drug discovery and improve annotation of protein funciton.
Experimental techniques commonly employed to determine the structure of protein complexes at atomic-scale resolution include X-ray crystallography nuclear magnetic resonance (NMR) spectroscopy, and cryo-electron microscopy (cryo-EM). Information about interface residues can also be obtained by alanine scanning mutagenesis experiments or various footprinting experiments, such as hydrogen/deuterium exchange or hydroxy radical footprinting. As they may come useful they have the problem of still being currently expensive and low throughput. That's why in silico techniques could prove useful to fill the gap and determine 3D structure and interactions, specially given current available knowledge, larger datasets and that GPU acceleration have enabled the training of deeper neural network architectures. Broadly, there are 3 categories for PPI site prediction: a method focused in Machine Learning; Structure-based methods and Sequence-based methods.
 
![overview](overview.png?raw=true)

For our approach we decided to do Random Forest. 
PRO AND CONS RF :warning:
## Random Forest
Random Forests is an ensemble method that combines several individual classification trees in the following way: from the original sample several bootstrap samples are drawn, and an unpruned classification tree is fitted to each bootstrap sample. The feature selection for each split in the classification tree is conducted from a small random subset of predictor variables (features). From the complete forest the status of the response variable is predicted as an average or majority vote of the predictions of all trees.
### RF backgrounds
### Features
Describe ours... :warning: :warning::warning::warning:
practicity using pdb and DSSP.
Backup feature importance. 

## ML tool: scikit-learn
[scikit-learn](https://scikit-learn.org/stable/) is a popular Python library used for machine learning tasks such as classification, regression, and clustering. It provides a range of tools and algorithms for data preprocessing, feature extraction, model selection, and model evaluation. For deep learning frameworks, popular choices include: [PyTorch](https://pytorch.org), [TensorFlow](https://www.tensorflow.org), and [Theano](http://deeplearning.net/software/theano/).
## DSSP
To extract secundary structure information we used [DSSP](#references).





## Materials and Methods

### Data extraction
In this project, we will be using the subset of the scPDB dataset generated by the PUResNET team as our primary training dataset. This dataset contains a large number of protein structures and their corresponding ligands, making it ideal for training a neural network-based model (5). They developed an independent training dataset, which is a subset of scPDB. First, they grouped each of the protein structures from scPDB according to the UniProt ID. Then, I calculated the Tanimoto coefficient. Second, they selected the longest sequenced protein structure from each UniProt ID cluster based on the Tanimoto coefficient. If the Tanimoto coefficient was equal to or greater than 80%, they considered it a similar structure. Finally, they performed manual inspection using PYMOL, and selected 5020 protein structures out of 16034.
Additionally, we used the BindingDB dataset as a simple visual validation set to evaluate the performance of our model on a separate set. From this, we extracted PDB files from their different subsets, such as articles, ChEMBL, and patents, to further diversify our training data and ensure that our model could generalize well to different types of protein-ligand interactions. By using a diverse range of datasets for training and validation, we aimed to develop a robust and accurate model for predicting protein-ligand binding sites.
The secondary structure information was extracted using DSSP (6). DSSP is a longstanding tool for calculating secondary structural descriptors of proteins from their structures.

### Features selection
It has been observed that a single protein feature alone is inadequate for predicting PPI, as it lacks sufficient information. Therefore, combining some of these features has been found to be a more effective approach for improving the performance of machine learning in PPI prediction.
The residue features we used for our model where: Coordinates: The 3D spatial position of the residue within the protein structure. Amino acid: The type of amino acid that makes up the residue. Binding atom: The atom within the residue that interacts with other molecules, such as ligands or other proteins. Sequence Entropy: A measure of the disorder or randomness of the residue's movement or conformation. Isoelectric point: it corresponds to the pH in which the residue is found with no charge. Hydrophobicity: This property arises due to the non-polar nature of the molecule or part of the molecule, which means it lacks an electrical charge or dipole moment that can interact with water's polar nature. Secondary structure: The local structural arrangement of the residue, such as alpha helices or beta sheets. Solvent accessible surface area (SASA): The area of the residue's surface that is accessible to solvent molecules, which can affect its interactions with other molecules. B-factor: A measure of the thermal motion or flexibility of the residue. Phi and psi angles: The angles of rotation around the peptide bond that connect the residue to its neighbors, which can affect the protein's overall structure. Alpha-carbon distance: The distance between the alpha-carbon atoms of the residue and its neighbors, which can also affect the protein's overall structure. These features were selected on the basis of availability and ease of use. Other features described in the literature are: Average relative ASA, Average backbone ASA, Average relative backbone ASA, Average backbone ASA, Average relative backbone ASA, Average non-polar ASA, Average relative non-polar ASA, Average polar ASA, Average relative polar ASA, Average depth index (DPX), Average protrusion index (CX), Minimal protrusion index, Maximal protrusion index, Maximal depth index (7), evolutionary conservation, protein expression, marginal essentiality, co-essentiality, 3D structure, MIPS functional catalog, position-specific scoring matrices (PSSM)s,  residue interface propensity (8).
### ML tool
Regarding Machine Learning and Random Forest we decided to use scikit-learn (9) which is a popular Python library used for machine learning tasks such as classification, regression, and clustering. It provides a range of tools and algorithms for data preprocessing, feature extraction, model selection, and model evaluation; Additionally is of easy use and comprehensive documentation. For deep learning frameworks, popular choices include: PyTorch, TensorFlow, and Theano. As a caveat we know that scikit-learn has the con of limited scalability, limited flexibility and features for deep learning; but for the  purposes of this project we determined it fitted our demands and the pros exceeded the cons. Mainly following the principle that if it can be done with machine learning rather than deep learning, we should, to save some computing power.
## Discussion
The method for prediction ended up using 4 models. Each of the models have a different weight given to the positive binding site prediction. We’ve found that using this can allow us to produce better results. The optimal weight overall was found to be between 6 and 7. These two weights work really well in the prediction of small-regular sized proteins. For larger proteins we use the models 8 and 10, which are supposed to work better for larger models, since they are more likely to classify the value as a binding site. These last two ended up not working as well as expected. If we were to develop a new model with a weight of 15 or more we might have that sweet spot for the prediction of larger proteins. We’ve saved some of the results we’ve obtained in the TESTS directory in the GitHub. There are results for both good and bad predictions. If we look into them we can see that there are some proteins such as 4W1Q, 5T2W or 1G4W which result in good approximations. The main reason is that either the predictions are close to the ligands, or because they are in a potential cavity. Not all results were good however. We can see in the protein with code 1A4V that when a protein is too small, the prediction tends to be almost the whole protein. Another example of a bad prediction is the one of the protein 6QA2, which the model is unable to predict a binding site for. 

The script written for managing the predictions has two ways for it to make a prediction. It accepts a first input which can be either a file or a PDB code. If it is a file, the script uses the pdb file in said directory and saves the protein prediction using the name of the pdb file in the same directory as the pdb file. If the file is a PDB code the model simply creates a folder in the directory where the model is being executed and downloads the PDB file as ‘protein.pdb’. It then finds the prediction and returns the pdb and the text file containing the potential binding sites. The second argument is optional. It accepts only pickles of models and it makes the prediction with the specified model in the argument. This is for an easier approach of more personalized predictions. It is important to note that it only accepts pdb files as input, and it will also return a pdb file as an output. Mol2 was used exclusively in the training for the extraction of the known binding sites, but it will not be accepted as input for the protein.

## Limitations
The models are not able to predict the binding site of proteins with a high level of complexity. We have tested on several PDB files which have a really complicated structure. Neither of the models that we’ve generated has produced any good results with large proteins. There are also some cases in which the presence of non-protein components in the aminoacid sequence produces some errors. For cases like this we’ve developed a work around that makes sure to only take residues. Nevertheless, this method is not perfect and still performs not perfect against all cases. There are still some cases in which the protein will not be able to extract the alpha carbons for the calculations. These cases are not that common however, so the feature extraction works most of the time. There is another potential issue that may arise. This problem results from a difference in the length of the protein. We’ve not found a way to solve this, since it occurs rarely too, and there is no clear indication as to why it occurs. Lastly, the models might result in better or worse results. Given the nature of machine learning, it will not be able to work on all protein families with the same degree of  confidence. The model was trained using by taking a sample of 2000 proteins from the original 5020 from the database, and only the ones that didn’t raise any errors were saved, leading to only keeping 616 proteins for the model. This was later divided more and more, so we might expect to have some protein families more represented than others in the training set. This is a main limitation that can be worked around if we were to use the whole 5020 proteins. Nevertheless, the results are promising and show that if we were to increase the model’s training feed, we might end up with better results in the future. 

## Conclusions
RF is a versatile machine learning method that can handle a variety of data types and is highly accurate and robust due to its ability to combine multiple decision trees. It is relatively easy to use and provides insights into feature importance, making it useful for both feature selection and data interpretation. However, it is important to note that RF is not always the best choice for every situation, and other machine learning methods may be more appropriate depending on the specific problem and data at hand. As per the models, we can conclude that they have proven to be fairly useful for the prediction of binding sites in a wide set of proteins with varying levels of accuracy. For most predictions, the most accurate ones are produced with models 6 and 7. Any other model has not proven to be useful in our own experience testing over different values. Overall, the field of protein-ligand binding prediction is rapidly advancing, with new methods and tools being developed to better understand and predict molecular interactions.

## [References](./README.md/#references)